{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "view-in-github",
                "colab_type": "text"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/Amey-Thakur/ZERO-SHOT-VIDEO-GENERATION/blob/main/Source%20Code/ZERO-SHOT-VIDEO-GENERATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "#\n",
                "<h1 align=\"center\">\ud83c\udfac Zero-Shot Video Generation</h1>\n",
                "<h3 align=\"center\"><i>Text-to-Video Synthesis via Temporal Latent Warping & Cross-Frame Attention</i></h3>\n",
                "\n",
                "<div align=\"center\">\n",
                "\n",
                "| **Author** | **Profiles** |\n",
                "|:---:|:---|\n",
                "| **Amey Thakur** | [![GitHub](https://img.shields.io/badge/GitHub-Amey--Thakur-181717?logo=github)](https://github.com/Amey-Thakur) [![ORCID](https://img.shields.io/badge/ORCID-0000--0001--5644--1575-A6CE39?logo=orcid)](https://orcid.org/0000-0001-5644-1575) [![Google Scholar](https://img.shields.io/badge/Google_Scholar-Amey_Thakur-4285F4?logo=google-scholar&logoColor=white)](https://scholar.google.ca/citations?user=0inooPgAAAAJ&hl=en) [![Kaggle](https://img.shields.io/badge/Kaggle-Amey_Thakur-20BEFF?logo=kaggle)](https://www.kaggle.com/ameythakur20) |\n",
                "\n",
                "---\n",
                "\n",
                "**Research Foundation:** Based on [Text2Video-Zero](https://arxiv.org/abs/2303.13439) by the Picsart AI Research (PAIR) team.\n",
                "\n",
                "\ud83d\ude80 **Live Demo:** [Hugging Face Space](https://huggingface.co/spaces/AmeyThakur/ZERO-SHOT-VIDEO-GENERATION) | \ud83c\udfac **Video Demo:** [YouTube](https://youtu.be/za9hId6UPoY) | \ud83d\udcbb **Repository:** [GitHub](https://github.com/Amey-Thakur/ZERO-SHOT-VIDEO-GENERATION)\n",
                "\n",
                "</div>\n",
                "\n",
                "## \ud83d\udcd6 Introduction\n",
                "\n",
                "> **Zero-shot video generation enables creating temporally consistent videos from text prompts without requiring any video-specific training.**\n",
                "\n",
                "This implementation utilizes the **Text2Video-Zero** framework, an architecture designed to leverage pre-trained text-to-image diffusion models for video synthesis. By applying temporal latent warping and global cross-frame attention, the pipeline ensures structural and appearance consistency across generated frames without additional fine-tuning on video datasets.\n",
                "\n",
                "### Core Methodology\n",
                "1.  **Temporal Latent Warping**: Ensures consistent motion dynamics by warping latent representations according to defined motion fields.\n",
                "2.  **Cross-Frame Attention**: Replaces standard self-attention with cross-frame attention, allowing each frame to reference the first frame to preserve identity and background details.\n",
                "3.  **Background Smoothing**: Optionally detects salient objects and applies specialized warping to the background to reduce temporal flickering."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup-heading"
            },
            "source": [
                "## \u2601\ufe0f Cloud Environment Setup\n",
                "Execute the following cell to prepare the execution environment. This script is designed for cross-platform compatibility, managing directory structures, dependencies, and neural weights for cloud providers such as **Google Colab** and **Kaggle**.\n",
                "\n",
                "### Automated Procedures\n",
                "1.  **Repository Synchronization**: Clones the core project and navigates to the source directory.\n",
                "2.  **Dependency Management**: Installs required libraries including `diffusers`, `transformers`, `einops`, and `kornia`.\n",
                "3.  **Asset Acquisition**: Retrieves auxiliary annotator weights (poses, depth, etc.) from Hugging Face for advanced inference modes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "cloud-setup-cell"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import shutil\n",
                "import subprocess\n",
                "\n",
                "# Environment detection for automated setup\n",
                "try:\n",
                "    shell = get_ipython()\n",
                "    IS_COLAB = 'google.colab' in str(shell)\n",
                "    IS_KAGGLE = \"kaggle\" in os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", \"\")\n",
                "except NameError:\n",
                "    IS_COLAB = IS_KAGGLE = False\n",
                "\n",
                "PROJECT_NAME = \"ZERO-SHOT-VIDEO-GENERATION\"\n",
                "print(f\"\ud83c\udf0d Detected Environment: {'Google Colab' if IS_COLAB else ('Kaggle' if IS_KAGGLE else 'Local/Custom')}\")\n",
                "\n",
                "def initialize_environment():\n",
                "    \"\"\"\n",
                "    Performs workspace initialization and dependency resolution.\n",
                "    \"\"\"\n",
                "    if IS_COLAB or IS_KAGGLE:\n",
                "        # Establish workspace root based on cloud provider\n",
                "        WORKDIR = \"/content\" if IS_COLAB else \"/kaggle/working\"\n",
                "        os.chdir(WORKDIR)\n",
                "        \n",
                "        # Clone repository if not present\n",
                "        if not os.path.exists(PROJECT_NAME):\n",
                "            print(f\"\u2b07\ufe0f Cloning {PROJECT_NAME}...\")\n",
                "            os.system(f\"git clone https://github.com/Amey-Thakur/{PROJECT_NAME}\")\n",
                "        \n",
                "        # Transition to source code directory\n",
                "        os.chdir(os.path.join(WORKDIR, PROJECT_NAME, \"Source Code\"))\n",
                "        \n",
                "        print(\"\ud83d\udee0\ufe0f Installing required neural engine dependencies...\")\n",
                "        os.system(\"pip install -q diffusers transformers accelerate einops kornia imageio imageio-ffmpeg moviepy tomesd decord safetensors huggingface_hub ipywidgets\")\n",
                "        \n",
                "        # Neural asset management (Annotators/CKPTS)\n",
                "        from huggingface_hub import hf_hub_download\n",
                "        annotators = {\n",
                "            \"body_pose_model.pth\": \"lllyasviel/Annotators\",\n",
                "            \"hand_pose_model.pth\": \"lllyasviel/Annotators\",\n",
                "            \"dpt_hybrid-midas-501f0c75.pt\": \"lllyasviel/Annotators\",\n",
                "            \"upernet_global_small.pth\": \"lllyasviel/Annotators\"\n",
                "        }\n",
                "        os.makedirs(\"annotator/ckpts\", exist_ok=True)\n",
                "        for f, repo in annotators.items():\n",
                "            target = os.path.join(\"annotator/ckpts\", f)\n",
                "            if not os.path.exists(target):\n",
                "                print(f\"\u2b07\ufe0f Downloading neural weight: {f}\")\n",
                "                path = hf_hub_download(repo_id=repo, filename=f)\n",
                "                shutil.copy(path, target)\n",
                "                \n",
                "    print(\"\u2705 Environment workspace established.\")\n",
                "\n",
                "initialize_environment()\n",
                "if os.getcwd() not in sys.path: \n",
                "    sys.path.append(os.getcwd())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "init-heading"
            },
            "source": [
                "## 1\ufe0f\u20e3 Framework Initialization\n",
                "\n",
                "This section initializes the primary diffusion model and configures hardware acceleration. The system uses **Mixed Precision (FP16)** when CUDA is available to optimize VRAM utilization and inference speed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "model-init-cell"
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "import gc\n",
                "import warnings\n",
                "import transformers\n",
                "import diffusers\n",
                "import importlib\n",
                "\n",
                "# Automatic module reloading to synchronize local disk changes with the active kernel\n",
                "import text_to_video_pipeline\n",
                "import model\n",
                "importlib.reload(text_to_video_pipeline)\n",
                "importlib.reload(model)\n",
                "from model import Model, ModelType\n",
                "\n",
                "# Quiet logging to maintain a clean terminal output focus\n",
                "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"diffusers\")\n",
                "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")\n",
                "transformers.logging.set_verbosity_error()\n",
                "diffusers.logging.set_verbosity_error()\n",
                "\n",
                "# Hardware Discovery and Precision Orchestration\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
                "\n",
                "print(f\"\ud83c\udfaf Computation Device: {device}\")\n",
                "print(f\"\u2699\ufe0f Mathematical Precision: {dtype}\")\n",
                "\n",
                "if device == \"cuda\":\n",
                "    vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
                "    print(f\"\ud83d\udcdf GPU Engine: {torch.cuda.get_device_name(0)} ({vram:.2f} GB Available)\")\n",
                "\n",
                "# Initialization of the Zero-Shot Video Pipeline\n",
                "print(\"\u23f3 Instantiating neural pipeline...\")\n",
                "video_model = model.Model(device=device, dtype=dtype)\n",
                "\n",
                "def clear_hardware_cache():\n",
                "    \"\"\"\n",
                "    Performs garbage collection and clears CUDA memory buffers.\n",
                "    \"\"\"\n",
                "    gc.collect()\n",
                "    if torch.cuda.is_available(): \n",
                "        torch.cuda.empty_cache()\n",
                "\n",
                "print(\"\u2705 Pipeline operational.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ui-heading"
            },
            "source": [
                "## 2\ufe0f\u20e3 Video Generation Studio\n",
                "\n",
                "The interface below facilitates the generation of temporally consistent video sequences. Users can select from curated presets or define custom prompts. Higher frame counts and resolutions will increase computation time and memory requirements."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "ui-inference-cell"
            },
            "outputs": [],
            "source": [
                "import ipywidgets as widgets\n",
                "from IPython.display import display, HTML, clear_output\n",
                "import base64\n",
                "\n",
                "# Curated Prompt Presets for structural validation\n",
                "EXPERIMENTAL_PRESETS = [\n",
                "    \"an astronaut waving the arm on the moon\",\n",
                "    \"a sloth surfing on a wakeboard\",\n",
                "    \"a cute cat walking on grass\",\n",
                "    \"a horse is galloping on a street\",\n",
                "    \"a gorilla dancing on times square\"\n",
                "]\n",
                "\n",
                "# UI component instantiation\n",
                "presets = widgets.Dropdown(\n",
                "    options=[(\"Select a curated prompt...\", \"\")] + [(p, p) for p in EXPERIMENTAL_PRESETS], \n",
                "    description='Presets:', \n",
                "    layout={'width': '600px'}\n",
                ")\n",
                "\n",
                "prompt_textarea = widgets.Textarea(\n",
                "    value='an astronaut waving the arm on the moon', \n",
                "    description='Prompt:', \n",
                "    layout={'width': '600px', 'height': '60px'}\n",
                ")\n",
                "\n",
                "frame_slider = widgets.IntSlider(\n",
                "    value=8, min=4, max=24, \n",
                "    description='Frames:', \n",
                "    layout={'width': '300px'}\n",
                ")\n",
                "\n",
                "resolution_dropdown = widgets.Dropdown(\n",
                "    options=[256, 512, 768], \n",
                "    value=512, \n",
                "    description='Resolution:', \n",
                "    layout={'width': '300px'}\n",
                ")\n",
                "\n",
                "generate_button = widgets.Button(\n",
                "    description='\ud83c\udfac Generate Video', \n",
                "    button_style='primary', \n",
                "    layout={'width': '200px'}\n",
                ")\n",
                "\n",
                "output_widget = widgets.Output()\n",
                "\n",
                "def handle_preset_selection(change):\n",
                "    \"\"\"\n",
                "    Synchronizes the prompt text area with selected preset.\n",
                "    \"\"\"\n",
                "    if change['new']:\n",
                "        prompt_textarea.value = change['new']\n",
                "presets.observe(handle_preset_selection, names='value')\n",
                "\n",
                "def initiate_synthesis(b):\n",
                "    \"\"\"\n",
                "    Triggers the text-to-video inference engine.\n",
                "    \"\"\"\n",
                "    generate_button.disabled = True\n",
                "    generate_button.description = \"Synthesizing...\"\n",
                "    \n",
                "    with output_widget: \n",
                "        clear_output()\n",
                "        print(\"\u23f3 Processing neural frames...\")\n",
                "        try:\n",
                "            # Execute inference via the Model wrapper\n",
                "            final_path = video_model.process_text2video(\n",
                "                prompt=prompt_textarea.value, \n",
                "                video_length=frame_slider.value, \n",
                "                resolution=resolution_dropdown.value, \n",
                "                motion_field_strength_x=12.0, \n",
                "                motion_field_strength_y=12.0, \n",
                "                seed=42, fps=4, path=\"output.mp4\"\n",
                "            )\n",
                "            \n",
                "            # Encode result for Jupyter display synchronization\n",
                "            if os.path.exists(final_path):\n",
                "                with open(final_path, \"rb\") as video_file: \n",
                "                    encoded_data = base64.b64encode(video_file.read()).decode()\n",
                "                \n",
                "                display(HTML(f'''\n",
                "                    <div align=\"center\" style=\"margin-top: 20px;\">\n",
                "                        <video width=\"{resolution_dropdown.value}\" controls autoplay loop style=\"border-radius: 12px; border: 1px solid #ddd; box-shadow: 0 4px 15px rgba(0,0,0,0.1);\">\n",
                "                            <source src=\"data:video/mp4;base64,{encoded_data}\" type=\"video/mp4\">\n",
                "                        </video>\n",
                "                    </div>\n",
                "                '''))\n",
                "            else:\n",
                "                print(\"\u26a0\ufe0f Output file generation failed. Check terminal logs.\")\n",
                "        \n",
                "        except Exception as synthesis_error: \n",
                "            print(f\"\u274c Synthesis Error: {synthesis_error}\")\n",
                "            \n",
                "    generate_button.disabled = False\n",
                "    generate_button.description = \"\ud83c\udfac Generate Video\"\n",
                "    clear_hardware_cache()\n",
                "\n",
                "generate_button.on_click(initiate_synthesis)\n",
                "\n",
                "# UI Layout Assembly\n",
                "header_header = widgets.HTML(\"<h3 style='text-align: center; width: 600px; margin-bottom: 20px; font-weight: 800;'>\ud83c\udfac Zero-Shot Video Studio</h3>\")\n",
                "interactive_elements = widgets.VBox([\n",
                "    header_header,\n",
                "    presets,\n",
                "    prompt_textarea,\n",
                "    widgets.HBox([frame_slider, resolution_dropdown], layout={'width': '600px', 'justify_content': 'space-between', 'margin_bottom': '10px'}),\n",
                "    widgets.HBox([generate_button], layout={'width': '600px', 'justify_content': 'center', 'margin_top': '10px'})\n",
                "], layout={'align_items': 'center'})\n",
                "\n",
                "studio_frame = widgets.VBox([interactive_elements], layout=widgets.Layout(\n",
                "    border='1px solid #e0e0e0', \n",
                "    padding='25px', \n",
                "    border_radius='15px', \n",
                "    margin='10px 0',\n",
                "    background_color='#ffffff',\n",
                "    box_shadow='0 10px 30px rgba(0,0,0,0.05)'\n",
                "))\n",
                "\n",
                "display(studio_frame)\n",
                "display(output_widget)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "references"
            },
            "source": [
                "## \ud83d\udcda Technical References\n",
                "1. **Text2Video-Zero**: [Picsart AI Research (PAIR) - arXiv:2303.13439](https://arxiv.org/abs/2303.13439)\n",
                "2. **Diffusers Framework**: [Hugging Face Documentation](https://huggingface.co/docs/diffusers/index)\n",
                "3. **Dreamlike Photoreal 2.0**: [Standard Diffusion Checkpoint](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0)\n",
                "\n",
                "---\n",
                "*Research Laboratory for Neural Video Synthesis*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}