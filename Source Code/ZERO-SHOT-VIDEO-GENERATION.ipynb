{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Amey-Thakur/ZERO-SHOT-VIDEO-GENERATION/blob/main/Source%20Code/ZERO-SHOT-VIDEO-GENERATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "#\n",
    "<h1 align=\"center\">\ud83c\udfac Zero-Shot Video Generation</h1>\n",
    "<h3 align=\"center\"><i>Text-to-Video Synthesis via Temporal Latent Warping & Cross-Frame Attention</i></h3>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| **Author** | **Profiles** |\n",
    "|:---:|:---|\n",
    "| **Amey Thakur** | [![GitHub](https://img.shields.io/badge/GitHub-Amey--Thakur-181717?logo=github)](https://github.com/Amey-Thakur) [![ORCID](https://img.shields.io/badge/ORCID-0000--0001--5644--1575-A6CE39?logo=orcid)](https://orcid.org/0000-0001-5644-1575) [![Google Scholar](https://img.shields.io/badge/Google_Scholar-Amey_Thakur-4285F4?logo=google-scholar&logoColor=white)](https://scholar.google.ca/citations?user=0inooPgAAAAJ&hl=en) [![Kaggle](https://img.shields.io/badge/Kaggle-Amey_Thakur-20BEFF?logo=kaggle)](https://www.kaggle.com/ameythakur20) |\n",
    "\n",
    "---\n",
    "\n",
    "**Research Foundation:** Based on [Text2Video-Zero](https://arxiv.org/abs/2303.13439) by the Picsart AI Research (PAIR) team.\n",
    "\n",
    "\ud83d\ude80 **Live Demo:** [Hugging Face Space](https://huggingface.co/spaces/AmeyThakur/ZERO-SHOT-VIDEO-GENERATION) | \ud83c\udfac **Video Demo:** [YouTube](https://youtu.be/za9hId6UPoY) | \ud83d\udcbb **Repository:** [GitHub](https://github.com/Amey-Thakur/ZERO-SHOT-VIDEO-GENERATION)\n",
    "\n",
    "</div>\n",
    "\n",
    "## \ud83d\udcd6 Introduction\n",
    "\n",
    "> **Zero-shot video generation enables creating temporally consistent videos from text prompts without requiring any video-specific training.**\n",
    "\n",
    "This implementation demonstrates the **Text2Video-Zero** framework, which transforms pre-trained Text-to-Image diffusion models into video generators by leveraging temporal latent warping and global cross-frame attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-heading"
   },
   "source": [
    "## \u2601\ufe0f Cloud Environment Setup\n",
    "Execute this cell to configure the environment. This script manages platform-agnostic paths and synchronizes required neural weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cloud-setup-cell"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    shell = get_ipython()\n",
    "    IS_COLAB = 'google.colab' in str(shell)\n",
    "    IS_KAGGLE = \"kaggle\" in os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", \"\")\n",
    "except NameError:\n",
    "    IS_COLAB = IS_KAGGLE = False\n",
    "\n",
    "PROJECT_NAME = \"ZERO-SHOT-VIDEO-GENERATION\"\n",
    "print(f\"\ud83c\udf0d Environment: {'Google Colab' if IS_COLAB else ('Kaggle' if IS_KAGGLE else 'Local/Custom')}\")\n",
    "\n",
    "def run_setup():\n",
    "    if IS_COLAB or IS_KAGGLE:\n",
    "        WORKDIR = \"/content\" if IS_COLAB else \"/kaggle/working\"\n",
    "        os.chdir(WORKDIR)\n",
    "        if not os.path.exists(PROJECT_NAME):\n",
    "            os.system(f\"git clone https://github.com/Amey-Thakur/{PROJECT_NAME}\")\n",
    "        os.chdir(os.path.join(WORKDIR, PROJECT_NAME, \"Source Code\"))\n",
    "        print(\"\ud83d\udee0\ufe0f Installing Dependencies...\")\n",
    "        os.system(\"pip install -q diffusers transformers accelerate einops kornia imageio imageio-ffmpeg moviepy tomesd decord safetensors huggingface_hub ipywidgets\")\n",
    "        \n",
    "        from huggingface_hub import hf_hub_download\n",
    "        annotators = {\n",
    "            \"body_pose_model.pth\": \"lllyasviel/Annotators\",\n",
    "            \"hand_pose_model.pth\": \"lllyasviel/Annotators\",\n",
    "            \"dpt_hybrid-midas-501f0c75.pt\": \"lllyasviel/Annotators\",\n",
    "            \"upernet_global_small.pth\": \"lllyasviel/Annotators\"\n",
    "        }\n",
    "        os.makedirs(\"annotator/ckpts\", exist_ok=True)\n",
    "        for f, repo in annotators.items():\n",
    "            target = os.path.join(\"annotator/ckpts\", f)\n",
    "            if not os.path.exists(target):\n",
    "                path = hf_hub_download(repo_id=repo, filename=f)\n",
    "                shutil.copy(path, target)\n",
    "    print(\"\u2705 Environment Ready.\")\n",
    "\n",
    "run_setup()\n",
    "if os.getcwd() not in sys.path: sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init-heading"
   },
   "source": [
    "## 1\ufe0f\u20e3 Model Initialization\n",
    "Initializing the diffusion pipeline and verifying hardware availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model-init-cell"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import warnings\n",
    "from model import Model, ModelType\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"diffusers\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "print(f\"\ud83c\udfaf Device: {device} | Precision: {dtype}\")\n",
    "if device == \"cuda\":\n",
    "    vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"\ud83d\udcdf GPU: {torch.cuda.get_device_name(0)} ({vram:.2f} GB VRAM)\")\n",
    "\n",
    "print(\"\u23f3 Loading Pipeline...\")\n",
    "model = Model(device=device, dtype=dtype)\n",
    "\n",
    "def cleanup_vram():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "print(\"\u2705 Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ui-heading"
   },
   "source": [
    "## 2\ufe0f\u20e3 Video Generation Studio\n",
    "Use the interface below to generate videos. The parameters allow for fine-tuning motion and resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ui-inference-cell"
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import base64\n",
    "\n",
    "EXAMPLES = [\n",
    "    \"an astronaut waving the arm on the moon\",\n",
    "    \"a sloth surfing on a wakeboard\",\n",
    "    \"a cute cat walking on grass\",\n",
    "    \"a horse is galloping on a street\",\n",
    "    \"a gorilla dancing on times square\"\n",
    "]\n",
    "\n",
    "presets = widgets.Dropdown(options=[(\"Select a preset...\", \"\")] + [(p, p) for p in EXAMPLES], description='Presets:', layout={'width': '600px'})\n",
    "prompt = widgets.Textarea(\n",
    "    value='an astronaut waving the arm on the moon', \n",
    "    description='Prompt:', \n",
    "    layout={'width': '600px', 'height': '60px'}\n",
    ")\n",
    "v_len = widgets.IntSlider(value=8, min=4, max=24, description='Frames:', layout={'width': '300px'})\n",
    "v_res = widgets.Dropdown(options=[256, 512, 768], value=512, description='Resolution:', layout={'width': '300px'})\n",
    "btn = widgets.Button(description='\ud83c\udfac Generate Video', button_style='primary', layout={'width': '200px'})\n",
    "out = widgets.Output() # Removing flex layout so progress bars render normally and don't break\n",
    "\n",
    "def on_preset_change(change):\n",
    "    if change['new']:\n",
    "        prompt.value = change['new']\n",
    "presets.observe(on_preset_change, names='value')\n",
    "\n",
    "def on_gen(b):\n",
    "    btn.disabled = True\n",
    "    btn.description = \"Processing...\"\n",
    "    with out: \n",
    "        clear_output()\n",
    "        print(\"\u23f3 Generating video...\")\n",
    "        try:\n",
    "            path = model.process_text2video(\n",
    "                prompt=prompt.value, video_length=v_len.value, resolution=v_res.value, \n",
    "                motion_field_strength_x=12.0, \n",
    "                motion_field_strength_y=12.0, \n",
    "                seed=42, fps=4, path=\"output.mp4\"\n",
    "            )\n",
    "            with open(path, \"rb\") as f: \n",
    "                data = f.read()\n",
    "            b64 = base64.b64encode(data).decode()\n",
    "            display(HTML(f'<div align=\"center\" style=\"margin-top: 20px;\"><video width=\"{v_res.value}\" controls autoplay loop style=\"border-radius: 8px; border: 1px solid #ddd;\"><source src=\"data:video/mp4;base64,{b64}\" type=\"video/mp4\"></video></div>'))\n",
    "        except Exception as e: \n",
    "            print(f\"\u274c Error: {e}\")\n",
    "    btn.disabled = False\n",
    "    btn.description = \"\ud83c\udfac Generate Video\"\n",
    "    cleanup_vram()\n",
    "\n",
    "btn.on_click(on_gen)\n",
    "\n",
    "header_html = widgets.HTML(\"<h3 style='text-align: center; width: 600px; margin-bottom: 20px;'>\ud83c\udfac Zero-Shot Video Generation</h3>\")\n",
    "btn_box = widgets.HBox([btn], layout={'width': '600px', 'justify_content': 'center', 'margin_top': '10px'})\n",
    "\n",
    "ui_content = widgets.VBox([\n",
    "    header_html,\n",
    "    presets,\n",
    "    prompt,\n",
    "    widgets.HBox([v_len, v_res], layout={'width': '600px', 'justify_content': 'space-between', 'margin_bottom': '10px'}),\n",
    "    btn_box\n",
    "], layout={'align_items': 'center'})\n",
    "\n",
    "ui_container = widgets.VBox([ui_content], layout=widgets.Layout(\n",
    "    border='1px solid #cfcfcf', \n",
    "    padding='20px', \n",
    "    border_radius='10px', \n",
    "    margin='10px 0',\n",
    "    background_color='#fbfbfb'\n",
    "))\n",
    "\n",
    "display(ui_container)\n",
    "display(out) # Output is now completely outside the box to prevent layout breaking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "references"
   },
   "source": [
    "## \ud83d\udcda References\n",
    "1. **Text2Video-Zero**: [arXiv:2303.13439](https://arxiv.org/abs/2303.13439)\n",
    "2. **Dreamlike Photoreal 2.0**: [HuggingFace](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0)\n",
    "\n",
    "---\n",
    "*Amey Thakur | University of Windsor*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}