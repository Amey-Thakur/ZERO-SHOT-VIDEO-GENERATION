{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Amey-Thakur/ZERO-SHOT-VIDEO-GENERATION/blob/main/Source%20Code/ZERO-SHOT-VIDEO-GENERATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "#\n",
    "<h1 align=\"center\">\ud83c\udfac Zero-Shot Video Generation</h1>\n",
    "<h3 align=\"center\"><i>Text-to-Video Synthesis via Temporal Latent Warping & Cross-Frame Attention</i></h3>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| **Author** | **Profiles** |\n",
    "|:---:|:---|\n",
    "| **Amey Thakur** | [![GitHub](https://img.shields.io/badge/GitHub-Amey--Thakur-181717?logo=github)](https://github.com/Amey-Thakur) [![ORCID](https://img.shields.io/badge/ORCID-0000--0001--5644--1575-A6CE39?logo=orcid)](https://orcid.org/0000-0001-5644-1575) [![Google Scholar](https://img.shields.io/badge/Google_Scholar-Amey_Thakur-4285F4?logo=google-scholar&logoColor=white)](https://scholar.google.ca/citations?user=0inooPgAAAAJ&hl=en) [![Kaggle](https://img.shields.io/badge/Kaggle-Amey_Thakur-20BEFF?logo=kaggle)](https://www.kaggle.com/ameythakur20) |\n",
    "\n",
    "---\n",
    "\n",
    "**Research Foundation:** Based on [Text2Video-Zero](https://arxiv.org/abs/2303.13439) by the Picsart AI Research (PAIR) team.\n",
    "\n",
    "\ud83d\ude80 **Live Demo:** [Hugging Face Space](https://huggingface.co/spaces/AmeyThakur/ZERO-SHOT-VIDEO-GENERATION) | \ud83c\udfac **Video Demo:** [YouTube](https://youtu.be/za9hId6UPoY) | \ud83d\udcbb **Repository:** [GitHub](https://github.com/Amey-Thakur/ZERO-SHOT-VIDEO-GENERATION)\n",
    "\n",
    "</div>\n",
    "\n",
    "## \ud83d\udcd6 Introduction\n",
    "\n",
    "> **Zero-shot video generation enables creating temporally consistent videos from text prompts without requiring any video-specific training.**\n",
    "\n",
    "This implementation demonstrates the **Text2Video-Zero** framework, which transforms pre-trained Text-to-Image diffusion models into video generators by leveraging temporal latent warping and global cross-frame attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-heading"
   },
   "source": [
    "## \u2601\ufe0f Cloud Environment Setup\n",
    "Execute this cell to configure the environment. This script manages platform-agnostic paths and synchronizes required neural weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cloud-setup-cell"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    shell = get_ipython()\n",
    "    IS_COLAB = 'google.colab' in str(shell)\n",
    "    IS_KAGGLE = \"kaggle\" in os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", \"\")\n",
    "except NameError:\n",
    "    IS_COLAB = IS_KAGGLE = False\n",
    "\n",
    "PROJECT_NAME = \"ZERO-SHOT-VIDEO-GENERATION\"\n",
    "print(f\"\ud83c\udf0d Environment: {'Google Colab' if IS_COLAB else ('Kaggle' if IS_KAGGLE else 'Local/Custom')}\")\n",
    "\n",
    "def run_setup():\n",
    "    if IS_COLAB or IS_KAGGLE:\n",
    "        WORKDIR = \"/content\" if IS_COLAB else \"/kaggle/working\"\n",
    "        os.chdir(WORKDIR)\n",
    "        if not os.path.exists(PROJECT_NAME):\n",
    "            os.system(f\"git clone https://github.com/Amey-Thakur/{PROJECT_NAME}\")\n",
    "        os.chdir(os.path.join(WORKDIR, PROJECT_NAME, \"Source Code\"))\n",
    "        print(\"\ud83d\udee0\ufe0f Installing Dependencies...\")\n",
    "        os.system(\"pip install -q diffusers transformers accelerate einops kornia imageio imageio-ffmpeg moviepy tomesd decord safetensors huggingface_hub ipywidgets\")\n",
    "        \n",
    "        from huggingface_hub import hf_hub_download\n",
    "        annotators = {\n",
    "            \"body_pose_model.pth\": \"lllyasviel/Annotators\",\n",
    "            \"hand_pose_model.pth\": \"lllyasviel/Annotators\",\n",
    "            \"dpt_hybrid-midas-501f0c75.pt\": \"lllyasviel/Annotators\",\n",
    "            \"upernet_global_small.pth\": \"lllyasviel/Annotators\"\n",
    "        }\n",
    "        os.makedirs(\"annotator/ckpts\", exist_ok=True)\n",
    "        for f, repo in annotators.items():\n",
    "            target = os.path.join(\"annotator/ckpts\", f)\n",
    "            if not os.path.exists(target):\n",
    "                path = hf_hub_download(repo_id=repo, filename=f)\n",
    "                shutil.copy(path, target)\n",
    "    print(\"\u2705 Environment Ready.\")\n",
    "\n",
    "run_setup()\n",
    "if os.getcwd() not in sys.path: sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init-heading"
   },
   "source": [
    "## 1\ufe0f\u20e3 Model Initialization\n",
    "Initializing the diffusion pipeline and verifying hardware availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model-init-cell"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import warnings\n",
    "from model import Model, ModelType\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"diffusers\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "print(f\"\ud83c\udfaf Device: {device} | Precision: {dtype}\")\n",
    "if device == \"cuda\":\n",
    "    vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"\ud83d\udcdf GPU: {torch.cuda.get_device_name(0)} ({vram:.2f} GB VRAM)\")\n",
    "\n",
    "print(\"\u23f3 Loading Pipeline...\")\n",
    "model = Model(device=device, dtype=dtype)\n",
    "\n",
    "def cleanup_vram():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "print(\"\u2705 Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ui-heading"
   },
   "source": [
    "## 2\ufe0f\u20e3 Video Generation Studio\n",
    "Use the interface below to generate videos. The parameters allow for fine-tuning motion and resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ui-inference-cell"
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import base64\n",
    "\n",
    "EXAMPLES = [\n",
    "    \"an astronaut waving the arm on the moon\",\n",
    "    \"a sloth surfing on a wakeboard\",\n",
    "    \"a cute cat walking on grass\",\n",
    "    \"a horse is galloping on a street\",\n",
    "    \"a gorilla dancing on times square\"\n",
    "]\n",
    "\n",
    "style = HTML(\"\"\"<style>\n",
    "    .studio-box { padding: 25px; background-color: #1e1e2e; border-radius: 12px; border: 1px solid #313244; color: #cdd6f4; }\n",
    "    .gen-btn { background: #89b4fa !important; color: #11111b !important; font-weight: bold !important; height: 45px !important; border-radius: 8px !important; }\n",
    "    .widget-label { font-weight: 500; color: #a6adc8; }\n",
    "    .studio-title { font-size: 1.4rem; font-weight: 700; margin-bottom: 15px; color: #f5e0dc; }\n",
    "</style>\"\"\")\n",
    "\n",
    "presets = widgets.Dropdown(options=[(\"Select a visual preset...\", \"\")] + [(p, p) for p in EXAMPLES], description='Presets', layout={'width': '100%'})\n",
    "prompt = widgets.Textarea(value='an astronaut waving the arm on the moon', description='Prompt', layout={'width': '100%', 'height': '80px'})\n",
    "v_len = widgets.IntSlider(value=8, min=4, max=24, description='Frames', layout={'width': '48%'})\n",
    "v_res = widgets.Dropdown(options=[256, 512, 768], value=512, description='Resolution', layout={'width': '48%'})\n",
    "v_mot = widgets.FloatSlider(value=12.0, min=0.0, max=30.0, description='Motion', layout={'width': '48%'})\n",
    "v_stp = widgets.IntSlider(value=50, min=10, max=100, description='Steps', layout={'width': '48%'})\n",
    "v_fps = widgets.IntSlider(value=4, min=1, max=12, description='FPS', layout={'width': '48%'})\n",
    "v_seed = widgets.IntText(value=42, description='Seed', layout={'width': '48%'})\n",
    "btn = widgets.Button(description='Generate Video', layout={'width': '100%'}); btn.add_class('gen-btn')\n",
    "out = widgets.Output()\n",
    "\n",
    "def on_gen(b):\n",
    "    btn.disabled = True; btn.description = \"Processing synthesis...\"\n",
    "    with out: \n",
    "        clear_output(); print(\"\u23f3 Generating...\")\n",
    "        try:\n",
    "            path = model.process_text2video(prompt=prompt.value, video_length=v_len.value, resolution=v_res.value, \n",
    "                                           motion_field_strength_x=v_mot.value, motion_field_strength_y=v_mot.value, \n",
    "                                           seed=v_seed.value, fps=v_fps.value, path=\"output.mp4\")\n",
    "            with open(path, \"rb\") as f: data = f.read()\n",
    "            b64 = base64.b64encode(data).decode()\n",
    "            display(HTML(f'<div style=\"padding:15px;background:#181825;border-radius:10px;text-align:center;\"><video width=\"100%\" controls autoplay loop><source src=\"data:video/mp4;base64,{b64}\" type=\"video/mp4\"></video></div>'))\n",
    "        except Exception as e: print(f\"\u274c Error: {e}\")\n",
    "    btn.disabled = False; btn.description = \"Generate Video\"; cleanup_vram()\n",
    "\n",
    "presets.observe(lambda c: setattr(prompt, 'value', c['new']) if c['new'] else None, 'value')\n",
    "btn.on_click(on_gen)\n",
    "\n",
    "ui = widgets.VBox([widgets.HTML(\"<div class='studio-title'>\ud83c\udfac Video Generation Studio</div>\"), presets, prompt, \n",
    "                  widgets.HBox([v_len, v_res]), widgets.HBox([v_mot, v_stp]), widgets.HBox([v_fps, v_seed]), btn])\n",
    "ui.add_class('studio-box')\n",
    "display(style, ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "references"
   },
   "source": [
    "## \ud83d\udcda References\n",
    "1. **Text2Video-Zero**: [arXiv:2303.13439](https://arxiv.org/abs/2303.13439)\n",
    "2. **Dreamlike Photoreal 2.0**: [HuggingFace](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0)\n",
    "\n",
    "---\n",
    "*Amey Thakur | University of Windsor*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}