{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Amey-Thakur/ZERO-SHOT-VIDEO-GENERATION/blob/main/Source%20Code/Zero_Shot_Video_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "#\n",
    "<h1 align=\"center\">\ud83c\udfac Zero-Shot Video Generation</h1>\n",
    "<h3 align=\"center\"><i>Text-to-Video Synthesis via Temporal Latent Warping & Cross-Frame Attention</i></h3>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| **Author** | **Profiles** |\n",
    "|:---:|:---|\n",
    "| **Amey Thakur** | [![GitHub](https://img.shields.io/badge/GitHub-Amey--Thakur-181717?logo=github)](https://github.com/Amey-Thakur) [![ORCID](https://img.shields.io/badge/ORCID-0000--0001--5644--1575-A6CE39?logo=orcid)](https://orcid.org/0000-0001-5644-1575) [![Google Scholar](https://img.shields.io/badge/Google_Scholar-Amey_Thakur-4285F4?logo=google-scholar&logoColor=white)](https://scholar.google.ca/citations?user=0inooPgAAAAJ&hl=en) [![Kaggle](https://img.shields.io/badge/Kaggle-Amey_Thakur-20BEFF?logo=kaggle)](https://www.kaggle.com/ameythakur20) |\n",
    "\n",
    "---\n",
    "\n",
    "**Research Foundation:** Based on [Text2Video-Zero](https://arxiv.org/abs/2303.13439) by the Picsart AI Research (PAIR) team.\n",
    "\n",
    "\ud83d\ude80 **Live Demo:** [Hugging Face Space](https://huggingface.co/spaces/AmeyThakur/ZERO-SHOT-VIDEO-GENERATION) | \ud83c\udfac **Video Demo:** [YouTube](https://youtu.be/za9hId6UPoY) | \ud83d\udcbb **Repository:** [GitHub](https://github.com/Amey-Thakur/ZERO-SHOT-VIDEO-GENERATION)\n",
    "\n",
    "</div>\n",
    "\n",
    "## \ud83d\udcd6 Introduction\n",
    "\n",
    "> **An audio-visual deepfake is when temporally consistent synthetic content is generated without requiring person-specific training.**\n",
    "\n",
    "This research implementation demonstrates **Text2Video-Zero**, a state-of-the-art framework that transforms pre-trained Text-to-Image (T2I) diffusion models into zero-shot video generators. By enforcing temporal consistency through latent manipulation and attention synchronization, we can synthesize high-quality videos without any video-specific fine-tuning.\n",
    "\n",
    "### Core Innovations:\n",
    "1.  **Temporal Latent Warping**: Enforces geometric consistency by warping latents along a motion field.\n",
    "2.  **Global Cross-Frame Attention**: Synchronizes object identity by making all frames attend to the appearance of the first frame.\n",
    "3.  **Background Smoothing**: Separates foreground dynamics from background stability to reduce pixel flickering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-heading"
   },
   "source": [
    "## \u2601\ufe0f Cloud Environment Setup\n",
    "Execute the following cell to configure your environment. This script is **fail-proof** and **multi-tier fallback** optimized.\n",
    "\n",
    "### Operational Workflow:\n",
    "1.  **Environment Detection**: Automatically detects **Kaggle**, **Colab**, or **Local** execution.\n",
    "2.  **Robust Cloning**: Attempts GitHub first, with a fallback to **Hugging Face Spaces** if LFS budget is exceeded.\n",
    "3.  **Asset Synchronization**: Automatically downloads missing neural weights (model checkpoints and annotators) from the Hugging Face Hub if they aren't provided in the local filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cloud-setup-cell"
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import numpy as np\n",
    "import base64\n",
    "\n",
    "# \u2500\u2500 Presets \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "EXAMPLES = [\n",
    "    \"an astronaut waving the arm on the moon\",\n",
    "    \"a sloth surfing on a wakeboard\",\n",
    "    \"a cute cat walking on grass\",\n",
    "    \"a horse is galloping on a street\",\n",
    "    \"a gorilla dancing on times square\"\n",
    "]\n",
    "\n",
    "# \u2500\u2500 UI Style (Premium Scholarly Theme) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "style = HTML(\"\"\"\n",
    "<style>\n",
    "    .studio-container {\n",
    "        padding: 30px;\n",
    "        background-color: #1e1e2e;\n",
    "        border-radius: 15px;\n",
    "        border: 1px solid #313244;\n",
    "        box-shadow: 0 10px 30px rgba(0,0,0,0.5);\n",
    "        color: #cdd6f4;\n",
    "        font-family: 'Inter', system-ui, -apple-system, sans-serif;\n",
    "    }\n",
    "    .widget-label { font-weight: 600; color: #a6adc8; margin-bottom: 5px; }\n",
    "    .custom-button {\n",
    "        background: linear-gradient(135deg, #89b4fa 0%, #74c7ec 100%) !important;\n",
    "        border-radius: 8px !important;\n",
    "        color: #11111b !important;\n",
    "        font-weight: 700 !important;\n",
    "        transition: all 0.3s ease !important;\n",
    "        height: 45px !important;\n",
    "        border: none !important;\n",
    "    }\n",
    "    .custom-button:hover {\n",
    "        transform: translateY(-2px);\n",
    "        box-shadow: 0 5px 15px rgba(137, 180, 250, 0.4);\n",
    "    }\n",
    "    .widget-dropdown > select, .widget-textarea > textarea, .widget-text > input {\n",
    "        background-color: #313244 !important;\n",
    "        color: #cdd6f4 !important;\n",
    "        border: 1px solid #45475a !important;\n",
    "        border-radius: 6px !important;\n",
    "        padding: 8px !important;\n",
    "    }\n",
    "    .jupyter-widgets.widget-slider .ui-slider-range { background: #89b4fa !important; }\n",
    "    .section-title {\n",
    "        font-size: 1.5rem;\n",
    "        font-weight: 800;\n",
    "        margin-bottom: 20px;\n",
    "        color: #f5e0dc;\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "        gap: 10px;\n",
    "    }\n",
    "</style>\n",
    "\"\"\")\n",
    "\n",
    "# \u2500\u2500 UI Components \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "header = widgets.HTML(\"<div class='section-title'>\ud83c\udfac Video Generation Studio</div>\")\n",
    "\n",
    "preset_dropdown = widgets.Dropdown(\n",
    "    options=[(\"Select a creative preset...\", \"\")] + [(p, p) for p in EXAMPLES],\n",
    "    description='<b>Style Presets</b>',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='100%', margin='0 0 15px 0')\n",
    ")\n",
    "\n",
    "prompt_input = widgets.Textarea(\n",
    "    value='an astronaut waving the arm on the moon',\n",
    "    placeholder='Describe your cinematic vision...',\n",
    "    description='<b>Prompt</b>',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='100%', height='100px', margin='0 0 20px 0')\n",
    ")\n",
    "\n",
    "# Sliders and Number Inputs\n",
    "slider_style = {'description_width': '140px'}\n",
    "length_slider = widgets.IntSlider(value=8, min=4, max=24, step=1, description='Video Length', style=slider_style, layout=widgets.Layout(width='48%'))\n",
    "res_dropdown = widgets.Dropdown(options=[256, 512, 768], value=512, description='Resolution', style=slider_style, layout=widgets.Layout(width='48%'))\n",
    "\n",
    "motion_slider = widgets.FloatSlider(value=12.0, min=0.0, max=30.0, step=0.5, description='Motion Dynamics', style=slider_style, layout=widgets.Layout(width='48%'))\n",
    "steps_slider = widgets.IntSlider(value=50, min=10, max=100, description='Inference Steps', style=slider_style, layout=widgets.Layout(width='48%'))\n",
    "\n",
    "fps_slider = widgets.IntSlider(value=4, min=1, max=12, description='Frames Per Second', style=slider_style, layout=widgets.Layout(width='48%'))\n",
    "seed_input = widgets.IntText(value=42, description='Random Seed', style=slider_style, layout=widgets.Layout(width='48%'))\n",
    "\n",
    "process_btn = widgets.Button(\n",
    "    description='\ud83d\ude80 GENERATE CINEMATIC VIDEO',\n",
    "    layout=widgets.Layout(width='100%', height='50px', margin='20px 0 0 0')\n",
    ")\n",
    "process_btn.add_class('custom-button')\n",
    "\n",
    "output_video = widgets.Output(layout=widgets.Layout(margin='20px 0 0 0'))\n",
    "\n",
    "# \u2500\u2500 Logic \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def on_preset_change(change):\n",
    "    if change['new']:\n",
    "        prompt_input.value = change['new']\n",
    "\n",
    "preset_dropdown.observe(on_preset_change, names='value')\n",
    "\n",
    "def run_generation(b):\n",
    "    process_btn.disabled = True\n",
    "    process_btn.description = \"\u23f3 SYNTHESIZING NEURONS...\"\n",
    "    \n",
    "    with output_video:\n",
    "        clear_output()\n",
    "        print(\"\\n\u26a1 Initializing Text-to-Video Pipeline...\")\n",
    "        \n",
    "        output_path = \"output.mp4\"\n",
    "        try:\n",
    "            video_path = model.process_text2video(\n",
    "                prompt=prompt_input.value,\n",
    "                video_length=length_slider.value,\n",
    "                resolution=res_dropdown.value,\n",
    "                motion_field_strength_x=motion_slider.value,\n",
    "                motion_field_strength_y=motion_slider.value,\n",
    "                seed=seed_input.value,\n",
    "                fps=fps_slider.value,\n",
    "                path=output_path\n",
    "            )\n",
    "            \n",
    "            with open(video_path, \"rb\") as f:\n",
    "                data = f.read()\n",
    "            b64 = base64.b64encode(data).decode()\n",
    "            html = f'''\n",
    "            <div style=\"background: #181825; padding: 20px; border-radius: 12px; border: 1px solid #313244; text-align: center;\">\n",
    "                <h4 style=\"color: #a6e3a1; margin-bottom: 15px;\">\u2728 Synthesis Successful</h4>\n",
    "                <video width=\"100%\" style=\"border-radius: 8px; box-shadow: 0 4px 20px rgba(0,0,0,0.4);\" controls autoplay loop>\n",
    "                    <source src=\"data:video/mp4;base64,{b64}\" type=\"video/mp4\">\n",
    "                </video>\n",
    "            </div>\n",
    "            '''\n",
    "            display(HTML(html))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Error: {e}\")\n",
    "        \n",
    "        process_btn.disabled = False\n",
    "        process_btn.description = \"\ud83d\ude80 GENERATE CINEMATIC VIDEO\"\n",
    "        cleanup_vram()\n",
    "\n",
    "process_btn.on_click(run_generation)\n",
    "\n",
    "# \u2500\u2500 Final Assembly \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "ui_form = widgets.VBox([\n",
    "    header,\n",
    "    preset_dropdown,\n",
    "    prompt_input,\n",
    "    widgets.HBox([length_slider, res_dropdown], layout=widgets.Layout(justify_content='space-between')),\n",
    "    widgets.HBox([motion_slider, steps_slider], layout=widgets.Layout(justify_content='space-between')),\n",
    "    widgets.HBox([fps_slider, seed_input], layout=widgets.Layout(justify_content='space-between')),\n",
    "    process_btn\n",
    "], layout=widgets.Layout(width='700px', padding='30px'))\n",
    "\n",
    "ui_container = widgets.VBox([style, ui_form])\n",
    "ui_container.add_class('studio-container')\n",
    "\n",
    "display(ui_container)\n",
    "display(output_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init-heading"
   },
   "source": [
    "## 1\ufe0f\u20e3 Pipeline & HW Initialization\n",
    "We initialize the Text2Video architecture. A **GPU with at least 15GB VRAM** is recommended for high-resolution output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model-init-cell"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import warnings\n",
    "from diffusers import DDIMScheduler\n",
    "from model import Model, ModelType\n",
    "\n",
    "# Suppress the \"Flax classes are deprecated\" warning since we are using PyTorch\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"diffusers\")\n",
    "\n",
    "# \u2500\u2500 Hardware Diagnostics \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "print(f\"\ud83c\udfaf Computation Device: {device}\")\n",
    "print(f\"\ud83d\udc8e Precision Mode:    {dtype}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"\ud83d\udcdf GPU:               {torch.cuda.get_device_name(0)}\")\n",
    "    # FIX: Changed 'total_mem' to 'total_memory'\n",
    "    vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"\ud83d\udcca Total VRAM:        {vram:.2f} GB\")\n",
    "\n",
    "# \u2500\u2500 Architecture Initialization \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print(\"\u23f3 Loading Neural Networks (Zero-Shot Pipeline)...\")\n",
    "model = Model(device=device, dtype=dtype)\n",
    "\n",
    "def cleanup_vram():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\u2705 Pipeline Operational.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ui-heading"
   },
   "source": [
    "## 2\ufe0f\u20e3 Synthesis Studio\n",
    "Configure your parameters below. This interactive studio allows you to fine-tune the motion dynamics and temporal consistency of the generated video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ui-inference-cell"
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import numpy as np\n",
    "import base64\n",
    "\n",
    "# \u2500\u2500 Presets \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "EXAMPLES = [\n",
    "    \"an astronaut waving the arm on the moon\",\n",
    "    \"a sloth surfing on a wakeboard\",\n",
    "    \"a cute cat walking on grass\",\n",
    "    \"a horse is galloping on a street\",\n",
    "    \"a gorilla dancing on times square\"\n",
    "]\n",
    "\n",
    "# \u2500\u2500 UI Components \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "header = widgets.HTML(\"<h3>\ud83c\udfac Video Generation Controls</h3>\")\n",
    "\n",
    "preset_dropdown = widgets.Dropdown(\n",
    "    options=[(\"Select a preset...\", \"\")] + [(p, p) for p in EXAMPLES],\n",
    "    description='<b>Presets:</b>',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "prompt_input = widgets.Textarea(\n",
    "    value='an astronaut waving the arm on the moon',\n",
    "    placeholder='Type your creative prompt here...',\n",
    "    description='<b>Prompt:</b>',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px', height='80px')\n",
    ")\n",
    "\n",
    "length_slider = widgets.IntSlider(value=8, min=4, max=24, step=1, description='Video Length (Frames)')\n",
    "res_dropdown = widgets.Dropdown(options=[256, 512, 768], value=512, description='Resolution')\n",
    "motion_slider = widgets.FloatSlider(value=12.0, min=0.0, max=30.0, step=0.5, description='Motion Strength')\n",
    "steps_slider = widgets.IntSlider(value=50, min=10, max=100, description='Inference Steps')\n",
    "fps_slider = widgets.IntSlider(value=4, min=1, max=12, description='Playback FPS')\n",
    "seed_input = widgets.IntText(value=42, description='Seed')\n",
    "\n",
    "process_btn = widgets.Button(\n",
    "    description='\ud83d\ude80 Generate Video',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='500px', height='40px')\n",
    ")\n",
    "\n",
    "output_vram = widgets.Output()\n",
    "output_video = widgets.Output()\n",
    "\n",
    "# \u2500\u2500 Logic \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def on_preset_change(change):\n",
    "    if change['new']:\n",
    "        prompt_input.value = change['new']\n",
    "\n",
    "preset_dropdown.observe(on_preset_change, names='value')\n",
    "\n",
    "def run_generation(b):\n",
    "    process_btn.disabled = True\n",
    "    process_btn.description = \"\u23f3 Processing... Please wait...\"\n",
    "    \n",
    "    with output_video:\n",
    "        clear_output()\n",
    "        print(\"\\n\ud83d\ude80 Initiating generation sequence...\")\n",
    "        \n",
    "        output_path = \"output.mp4\"\n",
    "        try:\n",
    "            video_path = model.process_text2video(\n",
    "                prompt=prompt_input.value,\n",
    "                video_length=length_slider.value,\n",
    "                resolution=res_dropdown.value,\n",
    "                motion_field_strength_x=motion_slider.value,\n",
    "                motion_field_strength_y=motion_slider.value,\n",
    "                seed=seed_input.value,\n",
    "                fps=fps_slider.value,\n",
    "                path=output_path\n",
    "            )\n",
    "            \n",
    "            # Display Video\n",
    "            with open(video_path, \"rb\") as f:\n",
    "                data = f.read()\n",
    "            b64 = base64.b64encode(data).decode()\n",
    "            html = f'''\n",
    "            <div align=\"center\">\n",
    "                <br>\n",
    "                <h4>\u2728 Generation Complete!</h4>\n",
    "                <video width=\"{res_dropdown.value}\" controls autoplay loop>\n",
    "                    <source src=\"data:video/mp4;base64,{b64}\" type=\"video/mp4\">\n",
    "                </video>\n",
    "            </div>\n",
    "            '''\n",
    "            display(HTML(html))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Error during synthesis: {e}\")\n",
    "        \n",
    "        process_btn.disabled = False\n",
    "        process_btn.description = \"\ud83d\ude80 Generate Video\"\n",
    "        cleanup_vram()\n",
    "\n",
    "process_btn.on_click(run_generation)\n",
    "\n",
    "# \u2500\u2500 Layout \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "ui = widgets.VBox([\n",
    "    header,\n",
    "    preset_dropdown,\n",
    "    prompt_input,\n",
    "    widgets.HBox([length_slider, res_dropdown]),\n",
    "    widgets.HBox([motion_slider, steps_slider]),\n",
    "    widgets.HBox([fps_slider, seed_input]),\n",
    "    process_btn,\n",
    "    output_vram,\n",
    "    output_video\n",
    "])\n",
    "\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "references"
   },
   "source": [
    "## \ud83d\udcda References\n",
    "1. **Text2Video-Zero**: Khachatryan, L. et al. (2023). *Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators.* [arXiv:2303.13439](https://arxiv.org/abs/2303.13439)\n",
    "2. **Dreamlike Photoreal 2.0**: [HuggingFace](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0)\n",
    "3. **Diffusers Framework**: [GitHub](https://github.com/huggingface/diffusers)\n",
    "\n",
    "---\n",
    "\n",
    "*Research Project | University of Windsor | Authors: Amey Thakur*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}