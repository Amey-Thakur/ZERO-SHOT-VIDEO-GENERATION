{
    "cells":  [
                  {
                      "cell_type":  "markdown",
                      "metadata":  {
                                       "id":  "view-in-github",
                                       "colab_type":  "text"
                                   },
                      "source":  [
                                     "\u003ca href=\"https://colab.research.google.com/github/Amey-Thakur/ZERO-SHOT-VIDEO-GENERATION/blob/main/Source%20Code/Zero_Shot_Video_Generation.ipynb\" target=\"_parent\"\u003e\u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\u003c/a\u003e"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {
                                       "id":  "header"
                                   },
                      "source":  [
                                     "#\n",
                                     "\u003ch1 align=\"center\"\u003eüé¨ Zero-Shot Video Generation\u003c/h1\u003e\n",
                                     "\u003ch3 align=\"center\"\u003e\u003ci\u003eText-to-Video Synthesis via Temporal Latent Warping \u0026 Cross-Frame Attention\u003c/i\u003e\u003c/h3\u003e\n",
                                     "\n",
                                     "\u003cdiv align=\"center\"\u003e\n",
                                     "\n",
                                     "| **Author** | **Profiles** |\n",
                                     "|:---:|:---|\n",
                                     "| **Amey Thakur** | [![GitHub](https://img.shields.io/badge/GitHub-Amey--Thakur-181717?logo=github)](https://github.com/Amey-Thakur) [![ORCID](https://img.shields.io/badge/ORCID-0000--0001--5644--1575-A6CE39?logo=orcid)](https://orcid.org/0000-0001-5644-1575) [![Google Scholar](https://img.shields.io/badge/Google_Scholar-Amey_Thakur-4285F4?logo=google-scholar\u0026logoColor=white)](https://scholar.google.ca/citations?user=0inooPgAAAAJ\u0026hl=en) [![Kaggle](https://img.shields.io/badge/Kaggle-Amey_Thakur-20BEFF?logo=kaggle)](https://www.kaggle.com/ameythakur20) |\n",
                                     "\n",
                                     "---\n",
                                     "\n",
                                     "**Research Foundation:** Based on [Text2Video-Zero](https://arxiv.org/abs/2303.13439) by the Picsart AI Research (PAIR) team.\n",
                                     "\n",
                                     "üöÄ **Live Demo:** [Hugging Face Space](https://huggingface.co/spaces/AmeyThakur/ZERO-SHOT-VIDEO-GENERATION) | üé¨ **Video Demo:** [YouTube](https://youtu.be/za9hId6UPoY) | üíª **Repository:** [GitHub](https://github.com/Amey-Thakur/ZERO-SHOT-VIDEO-GENERATION)\n",
                                     "\n",
                                     "\u003c/div\u003e\n",
                                     "\n",
                                     "## üìñ Introduction\n",
                                     "\n",
                                     "\u003e **Zero-shot video generation enables creating temporally consistent videos from text prompts without requiring any video-specific training.**\n",
                                     "\n",
                                     "This notebook implements the **Text2Video-Zero** framework, which transforms a pre-trained Stable Diffusion model into a video generator through:\n",
                                     "1.  **Temporal Latent Warping**: Ensuring geometric consistency by warping latents along a motion field.\n",
                                     "2.  **Global Cross-Frame Attention**: Synchronizing object appearance by making all frames attend to the first frame\u0027s appearance.\n",
                                     "3.  **Background Smoothing**: (Optional) Applying masks to separate foreground motion from background stability."
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {
                                       "id":  "setup-heading"
                                   },
                      "source":  [
                                     "## ‚òÅÔ∏è Cloud Environment Setup\n",
                                     "Execute the following cell to configure your environment. This script is designed to be **fail-proof** and **platform-agnostic**.\n",
                                     "\n",
                                     "### Features:\n",
                                     "1.  **Runtime Detection**: Automatically configures paths for **Kaggle**, **Colab**, or **Local** execution.\n",
                                     "2.  **Robust Cloning**: Attempts GitHub first, with a fallback to a **Hugging Face Mirror** if GitHub is unreachable.\n",
                                     "3.  **LFS Defense**: Automatically handles Git LFS budget issues by falling back to **Kagglehub** for model checkpoints.\n",
                                     "4.  **Tiered Asset Retrieval**: Prioritizes local mounts, then Kaggle datasets, then cloud downloads."
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {
                                       "id":  "cloud-setup-cell"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "import os\n",
                                     "import sys\n",
                                     "import shutil\n",
                                     "import subprocess\n",
                                     "\n",
                                     "# ‚îÄ‚îÄ Detect Environment ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                                     "try:\n",
                                     "    shell = get_ipython()\n",
                                     "    IS_COLAB = \u0027google.colab\u0027 in str(shell)\n",
                                     "    IS_KAGGLE = \"kaggle\" in os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", \"\")\n",
                                     "except NameError:\n",
                                     "    IS_COLAB = IS_KAGGLE = False\n",
                                     "\n",
                                     "PROJECT_NAME = \"ZERO-SHOT-VIDEO-GENERATION\"\n",
                                     "print(f\"üåç Environment: {\u0027Google Colab\u0027 if IS_COLAB else (\u0027Kaggle\u0027 if IS_KAGGLE else \u0027Local/Custom\u0027)}\")\n",
                                     "\n",
                                     "def run_setup():\n",
                                     "    if IS_COLAB or IS_KAGGLE:\n",
                                     "        WORKDIR = \"/content\" if IS_COLAB else \"/kaggle/working\"\n",
                                     "        os.chdir(WORKDIR)\n",
                                     "        \n",
                                     "        # 1. Clone Repository (with Fallback)\n",
                                     "        if not os.path.exists(PROJECT_NAME):\n",
                                     "            print(f\"‚¨áÔ∏è Cloning {PROJECT_NAME} from GitHub...\")\n",
                                     "            res = os.system(f\"git clone https://github.com/Amey-Thakur/{PROJECT_NAME}\")\n",
                                     "            \n",
                                     "            if res != 0 or not os.path.exists(PROJECT_NAME):\n",
                                     "                print(\"‚ö†Ô∏è GitHub Clone Failed. Falling back to Hugging Face Mirror...\")\n",
                                     "                os.system(f\"git clone https://huggingface.co/spaces/AmeyThakur/{PROJECT_NAME}\")\n",
                                     "        \n",
                                     "        os.chdir(os.path.join(WORKDIR, PROJECT_NAME, \"Source Code\"))\n",
                                     "        \n",
                                     "        # 2. Dependency Installation\n",
                                     "        print(\"üõ†Ô∏è Installing Dependencies...\")\n",
                                     "        os.system(\"pip install -q diffusers transformers accelerate einops kornia imageio imageio-ffmpeg moviepy tomesd decord safetensors kagglehub\")\n",
                                     "        \n",
                                     "        # 3. Model Fallback Check (Kagglehub)\n",
                                     "        # Check if local models directory exists and has content\n",
                                     "        models_trigger = \"models/dreamlike-photoreal-2.0/model_index.json\"\n",
                                     "        if not os.path.exists(models_trigger) or os.path.getsize(models_trigger) \u003c 100:\n",
                                     "            print(\"üì¶ Local models missing or LFS pointers detected. Using Kaggle Fallback...\")\n",
                                     "            import kagglehub\n",
                                     "            try:\n",
                                     "                # IMPORTANT: Replace with your actual public dataset handle when available\n",
                                     "                k_path = kagglehub.dataset_download(\"ameythakur20/zero-shot-video-gen\")\n",
                                     "                print(f\"‚úÖ Assets downloaded to {k_path}\")\n",
                                     "                \n",
                                     "                # Link models folder\n",
                                     "                k_models = os.path.join(k_path, \"models\")\n",
                                     "                if os.path.exists(k_models):\n",
                                     "                    if os.path.exists(\"models\"): \n",
                                     "                        if os.path.islink(\"models\"): os.unlink(\"models\")\n",
                                     "                        else: shutil.rmtree(\"models\")\n",
                                     "                    os.symlink(k_models, \"models\")\n",
                                     "                    print(\"üîó Models linked successfully.\")\n",
                                     "            except Exception as e:\n",
                                     "                print(f\"‚ö†Ô∏è Kagglehub download failed: {e}. Models will be downloaded from HF Hub on-demand.\")\n",
                                     "                \n",
                                     "    print(\"‚úÖ Environment Setup Complete.\")\n",
                                     "\n",
                                     "run_setup()\n",
                                     "\n",
                                     "# Add Source Code to path for module imports\n",
                                     "current_path = os.getcwd()\n",
                                     "if current_path not in sys.path:\n",
                                     "    sys.path.append(current_path)\n",
                                     "print(f\"üìç Python Path: {current_path}\")"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {
                                       "id":  "init-heading"
                                   },
                      "source":  [
                                     "## 1Ô∏è‚É£ Hardware \u0026 Model Initialization\n",
                                     "We initialize the Text2Video pipeline and verify GPU availability. For the best experience, a **GPU with at least 15GB VRAM** (like Colab\u0027s T4) is recommended."
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {
                                       "id":  "model-init-cell"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "import torch\n",
                                     "import gc\n",
                                     "from diffusers import DDIMScheduler\n",
                                     "from model import Model, ModelType\n",
                                     "from text_to_video_pipeline import TextToVideoPipeline\n",
                                     "from utils import CrossFrameAttnProcessor\n",
                                     "\n",
                                     "# ‚îÄ‚îÄ Hardware Diagnostics ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                                     "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                                     "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
                                     "print(f\"üéØ Computation Device: {device}\")\n",
                                     "print(f\"üíé Precision Mode:    {dtype}\")\n",
                                     "\n",
                                     "if device == \"cuda\":\n",
                                     "    print(f\"üìü GPU:               {torch.cuda.get_device_name(0)}\")\n",
                                     "    vram = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
                                     "    print(f\"üìä Total VRAM:        {vram:.2f} GB\")\n",
                                     "\n",
                                     "# ‚îÄ‚îÄ Pipeline Loading ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                                     "print(\"‚è≥ Loading Neural Networks (Stable Diffusion + T2V-Zero)...\")\n",
                                     "\n",
                                     "model_id = \"dreamlike-art/dreamlike-photoreal-2.0\"\n",
                                     "\n",
                                     "# Check for local weights\n",
                                     "local_path = os.path.abspath(os.path.join(os.getcwd(), \"models\", \"dreamlike-photoreal-2.0\"))\n",
                                     "load_path = local_path if os.path.exists(local_path) else model_id\n",
                                     "\n",
                                     "if os.path.exists(local_path):\n",
                                     "    print(f\"üü¢ Using local weights from {local_path}\")\n",
                                     "else:\n",
                                     "    print(f\"üåê Downloading weights from HuggingFace: {model_id}\")\n",
                                     "\n",
                                     "try:\n",
                                     "    # Load Model Wrapper\n",
                                     "    model = Model(device=device, dtype=dtype)\n",
                                     "    \n",
                                     "    # Configure Pipeline\n",
                                     "    # We initialize the Text2VideoPipeline directly for more control in the notebook\n",
                                     "    pipe = TextToVideoPipeline.from_pretrained(\n",
                                     "        load_path, \n",
                                     "        torch_dtype=dtype\n",
                                     "    ).to(device)\n",
                                     "    \n",
                                     "    # Set Scheduler\n",
                                     "    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
                                     "    \n",
                                     "    # Apply Temporal Consistency (Cross-Frame Attention)\n",
                                     "    attn_proc = CrossFrameAttnProcessor(unet_chunk_size=2)\n",
                                     "    pipe.unet.set_attn_processor(processor=attn_proc)\n",
                                     "    \n",
                                     "    print(\"‚úÖ Pipeline Operational. All components synchronized.\")\n",
                                     "except Exception as e:\n",
                                     "    print(f\"‚ùå Initialization Error: {e}\")\n",
                                     "\n",
                                     "def cleanup():\n",
                                     "    gc.collect()\n",
                                     "    if torch.cuda.is_available():\n",
                                     "        torch.cuda.empty_cache()"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {
                                       "id":  "inference-heading"
                                   },
                      "source":  [
                                     "## 2Ô∏è‚É£ Video Generation Interface\n",
                                     "Enter a descriptive prompt below. The system uses a **quality-enhancing prompt wrapper** to ensure cinematic results."
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {
                                       "id":  "inference-cell"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "import numpy as np\n",
                                     "import torchvision\n",
                                     "import imageio\n",
                                     "from IPython.display import HTML, display\n",
                                     "import base64\n",
                                     "\n",
                                     "def play_video(path, width=512):\n",
                                     "    with open(path, \"rb\") as f:\n",
                                     "        data = f.read()\n",
                                     "    b64 = base64.b64encode(data).decode()\n",
                                     "    return HTML(f\u0027\u003cvideo width=\"{width}\" controls autoplay loop\u003e\u003csource src=\"data:video/mp4;base64,{b64}\" type=\"video/mp4\"\u003e\u003c/video\u003e\u0027)\n",
                                     "\n",
                                     "def generate(\n",
                                     "    prompt,\n",
                                     "    video_length=8,\n",
                                     "    resolution=512,\n",
                                     "    seed=42,\n",
                                     "    steps=50,\n",
                                     "    motion_strength=12.0,\n",
                                     "    fps=4\n",
                                     "):\n",
                                     "    cleanup()\n",
                                     "    generator = torch.Generator(device=device).manual_seed(seed)\n",
                                     "    \n",
                                     "    print(f\"üé¨ Generating: \\\"{prompt}\\\"\")\n",
                                     "    \n",
                                     "    # Quality modifiers\n",
                                     "    added_prompt = \"high quality, HD, 8K, trending on artstation, high focus, dramatic lighting\"\n",
                                     "    negative_prompt = \"longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, deformed body, bloated, ugly, unrealistic\"\n",
                                     "    \n",
                                     "    full_prompt = [f\"{prompt}, {added_prompt}\"] * video_length\n",
                                     "    neg_prompt = [negative_prompt] * video_length\n",
                                     "    \n",
                                     "    result = pipe(\n",
                                     "        prompt=full_prompt,\n",
                                     "        negative_prompt=neg_prompt,\n",
                                     "        video_length=video_length,\n",
                                     "        height=resolution,\n",
                                     "        width=resolution,\n",
                                     "        num_inference_steps=steps,\n",
                                     "        guidance_scale=7.5,\n",
                                     "        motion_field_strength_x=motion_strength,\n",
                                     "        motion_field_strength_y=motion_strength,\n",
                                     "        generator=generator,\n",
                                     "        output_type=\"numpy\",\n",
                                     "        frame_ids=list(range(video_length))\n",
                                     "    )\n",
                                     "    \n",
                                     "    # Create Video\n",
                                     "    frames = result.images\n",
                                     "    video_frames = []\n",
                                     "    for frame in frames:\n",
                                     "        # The pipeline returns numpy for \u0027numpy\u0027 output_type, typically [F, H, W, C] if batched\n",
                                     "        # But check shape. From text_to_video_pipeline, images is typically List of PIL or np array\n",
                                     "        img = (frame * 255).astype(np.uint8)\n",
                                     "        video_frames.append(img)\n",
                                     "    \n",
                                     "    output_path = \"output.mp4\"\n",
                                     "    imageio.mimsave(output_path, video_frames, fps=fps)\n",
                                     "    return output_path\n",
                                     "\n",
                                     "# ‚îÄ‚îÄ Configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                                     "PROMPT = \"an astronaut waving the arm on the moon\"\n",
                                     "SEED = 42\n",
                                     "FRAMES = 8\n",
                                     "\n",
                                     "video_file = generate(PROMPT, video_length=FRAMES, seed=SEED)\n",
                                     "display(play_video(video_file))"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {
                                       "id":  "viz-heading"
                                   },
                      "source":  [
                                     "## 3Ô∏è‚É£ Advanced Visualization\n",
                                     "Analyze the temporal consistency by viewing the frames in a grid. Note the consistent identity of objects across the sequence."
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {
                                       "id":  "viz-cell"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "import matplotlib.pyplot as plt\n",
                                     "import cv2\n",
                                     "\n",
                                     "def show_frames(path):\n",
                                     "    cap = cv2.VideoCapture(path)\n",
                                     "    frames = []\n",
                                     "    while True:\n",
                                     "        ret, frame = cap.read()\n",
                                     "        if not ret: break\n",
                                     "        frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
                                     "    cap.release()\n",
                                     "    \n",
                                     "    fig, axes = plt.subplots(1, len(frames), figsize=(20, 5))\n",
                                     "    for i, f in enumerate(frames):\n",
                                     "        axes[i].imshow(f)\n",
                                     "        axes[i].axis(\u0027off\u0027)\n",
                                     "        axes[i].set_title(f\"Frame {i+1}\")\n",
                                     "    plt.show()\n",
                                     "\n",
                                     "show_frames(video_file)"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {
                                       "id":  "footer"
                                   },
                      "source":  [
                                     "---\n",
                                     "**Clean Environment Memory**"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {
                                       "id":  "cleanup-cell"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "cleanup()\n",
                                     "print(\"üßπ Memory cleared. System ready for next generation.\")"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "metadata":  {
                                       "id":  "references"
                                   },
                      "source":  [
                                     "## üìö References\n",
                                     "1. **Text2Video-Zero**: Khachatryan et al. [arXiv:2303.13439](https://arxiv.org/abs/2303.13439)\n",
                                     "2. **Dreamlike Photoreal 2.0**: [Hugging Face](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0)\n",
                                     "3. **Diffusers Library**: [GitHub](https://github.com/huggingface/diffusers)"
                                 ]
                  }
              ],
    "metadata":  {
                     "kernelspec":  {
                                        "display_name":  "Python 3",
                                        "language":  "python",
                                        "name":  "python3"
                                    },
                     "language_info":  {
                                           "name":  "python",
                                           "version":  "3.10.0"
                                       }
                 },
    "nbformat":  4,
    "nbformat_minor":  4
}
