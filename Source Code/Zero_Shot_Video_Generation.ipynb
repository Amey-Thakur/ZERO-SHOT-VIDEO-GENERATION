{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "view-in-github",
                "colab_type": "text"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/Amey-Thakur/ZERO-SHOT-VIDEO-GENERATION/blob/main/Source%20Code/Zero_Shot_Video_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "#\n",
                "<h1 align=\"center\">ğŸ¬ Zero-Shot Video Generation</h1>\n",
                "<h3 align=\"center\"><i>Text-to-Video Synthesis via Temporal Latent Warping & Cross-Frame Attention</i></h3>\n",
                "\n",
                "<div align=\"center\">\n",
                "\n",
                "| **Author** | **Profiles** |\n",
                "|:---:|:---|\n",
                "| **Amey Thakur** | [![GitHub](https://img.shields.io/badge/GitHub-Amey--Thakur-181717?logo=github)](https://github.com/Amey-Thakur) [![ORCID](https://img.shields.io/badge/ORCID-0000--0001--5644--1575-A6CE39?logo=orcid)](https://orcid.org/0000-0001-5644-1575) [![Google Scholar](https://img.shields.io/badge/Google_Scholar-Amey_Thakur-4285F4?logo=google-scholar&logoColor=white)](https://scholar.google.ca/citations?user=0inooPgAAAAJ&hl=en) [![Kaggle](https://img.shields.io/badge/Kaggle-Amey_Thakur-20BEFF?logo=kaggle)](https://www.kaggle.com/ameythakur20) |\n",
                "\n",
                "---\n",
                "\n",
                "**Research Foundation:** Based on [Text2Video-Zero](https://arxiv.org/abs/2303.13439) by the Picsart AI Research (PAIR) team.\n",
                "\n",
                "ğŸš€ **Live Demo:** [Hugging Face Space](https://huggingface.co/spaces/AmeyThakur/ZERO-SHOT-VIDEO-GENERATION) | ğŸ¬ **Video Demo:** [YouTube](https://youtu.be/za9hId6UPoY) | ğŸ’» **Repository:** [GitHub](https://github.com/Amey-Thakur/ZERO-SHOT-VIDEO-GENERATION)\n",
                "\n",
                "</div>\n",
                "\n",
                "## ğŸ“– Introduction\n",
                "\n",
                "> **An audio-visual deepfake is when temporally consistent synthetic content is generated without requiring person-specific training.**\n",
                "\n",
                "This research implementation demonstrates **Text2Video-Zero**, a state-of-the-art framework that transforms pre-trained Text-to-Image (T2I) diffusion models into zero-shot video generators. By enforcing temporal consistency through latent manipulation and attention synchronization, we can synthesize high-quality videos without any video-specific fine-tuning.\n",
                "\n",
                "### Core Innovations:\n",
                "1.  **Temporal Latent Warping**: Enforces geometric consistency by warping latents along a motion field.\n",
                "2.  **Global Cross-Frame Attention**: Synchronizes object identity by making all frames attend to the appearance of the first frame.\n",
                "3.  **Background Smoothing**: Separates foreground dynamics from background stability to reduce pixel flickering."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup-heading"
            },
            "source": [
                "## â˜ï¸ Cloud Environment Setup\n",
                "Execute the following cell to configure your environment. This script is **fail-proof** and **multi-tier fallback** optimized.\n",
                "\n",
                "### Operational Workflow:\n",
                "1.  **Environment Detection**: Automatically detects **Kaggle**, **Colab**, or **Local** execution.\n",
                "2.  **Robust Cloning**: Attempts GitHub first, with a fallback to **Hugging Face Spaces** if LFS budget is exceeded.\n",
                "3.  **Asset Synchronization**: Automatically downloads missing neural weights (model checkpoints and annotators) from the Hugging Face Hub if they aren't provided in the local filesystem."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "cloud-setup-cell"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import shutil\n",
                "import subprocess\n",
                "\n",
                "# â”€â”€ Detect Environment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "try:\n",
                "    shell = get_ipython()\n",
                "    IS_COLAB = 'google.colab' in str(shell)\n",
                "    IS_KAGGLE = \"kaggle\" in os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", \"\")\n",
                "except NameError:\n",
                "    IS_COLAB = IS_KAGGLE = False\n",
                "\n",
                "PROJECT_NAME = \"ZERO-SHOT-VIDEO-GENERATION\"\n",
                "print(f\"ğŸŒ Environment: {'Google Colab' if IS_COLAB else ('Kaggle' if IS_KAGGLE else 'Local/Custom')}\")\n",
                "\n",
                "def run_setup():\n",
                "    if IS_COLAB or IS_KAGGLE:\n",
                "        WORKDIR = \"/content\" if IS_COLAB else \"/kaggle/working\"\n",
                "        os.chdir(WORKDIR)\n",
                "        \n",
                "        # 1. Clone Repository (with Fallback)\n",
                "        if not os.path.exists(PROJECT_NAME):\n",
                "            print(f\"â¬‡ï¸ Cloning {PROJECT_NAME} from GitHub...\")\n",
                "            res = os.system(f\"git clone https://github.com/Amey-Thakur/{PROJECT_NAME}\")\n",
                "            \n",
                "            if res != 0 or not os.path.exists(PROJECT_NAME):\n",
                "                print(\"âš ï¸ GitHub Clone Failed or LFS Error. Falling back to Hugging Face Mirror...\")\n",
                "                os.system(f\"git clone https://huggingface.co/spaces/AmeyThakur/{PROJECT_NAME}\")\n",
                "        \n",
                "        os.chdir(os.path.join(WORKDIR, PROJECT_NAME, \"Source Code\"))\n",
                "        \n",
                "        # 2. Dependency Installation\n",
                "        print(\"ğŸ› ï¸ Installing Dependencies...\")\n",
                "        os.system(\"pip install -q diffusers transformers accelerate einops kornia imageio imageio-ffmpeg moviepy tomesd decord safetensors huggingface_hub ipywidgets\")\n",
                "        \n",
                "        # 3. Comprehensive Asset Synchronization\n",
                "        print(\"ğŸ“¦ Synchronizing neural weights...\")\n",
                "        from huggingface_hub import hf_hub_download\n",
                "        \n",
                "        # Annotator Checkpoints\n",
                "        annotators = {\n",
                "            \"body_pose_model.pth\": \"lllyasviel/Annotators\",\n",
                "            \"hand_pose_model.pth\": \"lllyasviel/Annotators\",\n",
                "            \"dpt_hybrid-midas-501f0c75.pt\": \"lllyasviel/Annotators\",\n",
                "            \"upernet_global_small.pth\": \"lllyasviel/Annotators\"\n",
                "        }\n",
                "        \n",
                "        os.makedirs(\"annotator/ckpts\", exist_ok=True)\n",
                "        for filename, repo_id in annotators.items():\n",
                "            target = os.path.join(\"annotator/ckpts\", filename)\n",
                "            if not os.path.exists(target) or os.path.getsize(target) < 1000:\n",
                "                print(f\"ğŸ“¥ Retrieving {filename}...\")\n",
                "                try:\n",
                "                    path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
                "                    shutil.copy(path, target)\n",
                "                except Exception as e:\n",
                "                    print(f\"âš ï¸ Download Failed: {e}\")\n",
                "                \n",
                "    print(\"âœ… Environment Setup Complete.\")\n",
                "\n",
                "run_setup()\n",
                "\n",
                "# Add Source Code to path for module imports\n",
                "current_path = os.getcwd()\n",
                "if current_path not in sys.path:\n",
                "    sys.path.append(current_path)\n",
                "print(f\"ğŸ“ Workspace: {current_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "init-heading"
            },
            "source": [
                "## 1ï¸âƒ£ Pipeline & HW Initialization\n",
                "We initialize the Text2Video architecture. A **GPU with at least 15GB VRAM** is recommended for high-resolution output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "model-init-cell"
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "import gc\n",
                "from diffusers import DDIMScheduler\n",
                "from model import Model, ModelType\n",
                "\n",
                "# â”€â”€ Hardware Diagnostics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
                "print(f\"ğŸ¯ Computation Device: {device}\")\n",
                "print(f\"ğŸ’ Precision Mode:    {dtype}\")\n",
                "\n",
                "if device == \"cuda\":\n",
                "    print(f\"ğŸ“Ÿ GPU:               {torch.cuda.get_device_name(0)}\")\n",
                "    vram = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
                "    print(f\"ğŸ“Š Total VRAM:        {vram:.2f} GB\")\n",
                "\n",
                "# â”€â”€ Architecture Initialization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "print(\"â³ Loading Neural Networks (Zero-Shot Pipeline)...\")\n",
                "model = Model(device=device, dtype=dtype)\n",
                "\n",
                "def cleanup_vram():\n",
                "    gc.collect()\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.empty_cache()\n",
                "\n",
                "print(\"âœ… Pipeline Operational.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ui-heading"
            },
            "source": [
                "## 2ï¸âƒ£ Synthesis Studio\n",
                "Configure your parameters below. This interactive studio allows you to fine-tune the motion dynamics and temporal consistency of the generated video."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "ui-inference-cell"
            },
            "outputs": [],
            "source": [
                "import ipywidgets as widgets\n",
                "from IPython.display import display, HTML, clear_output\n",
                "import numpy as np\n",
                "import base64\n",
                "\n",
                "# â”€â”€ Presets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "EXAMPLES = [\n",
                "    \"an astronaut waving the arm on the moon\",\n",
                "    \"a sloth surfing on a wakeboard\",\n",
                "    \"a cute cat walking on grass\",\n",
                "    \"a horse is galloping on a street\",\n",
                "    \"a gorilla dancing on times square\"\n",
                "]\n",
                "\n",
                "# â”€â”€ UI Components â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "header = widgets.HTML(\"<h3>ğŸ¬ Video Generation Controls</h3>\")\n",
                "\n",
                "preset_dropdown = widgets.Dropdown(\n",
                "    options=[(\"Select a preset...\", \"\")] + [(p, p) for p in EXAMPLES],\n",
                "    description='<b>Presets:</b>',\n",
                "    style={'description_width': 'initial'},\n",
                "    layout=widgets.Layout(width='500px')\n",
                ")\n",
                "\n",
                "prompt_input = widgets.Textarea(\n",
                "    value='an astronaut waving the arm on the moon',\n",
                "    placeholder='Type your creative prompt here...',\n",
                "    description='<b>Prompt:</b>',\n",
                "    style={'description_width': 'initial'},\n",
                "    layout=widgets.Layout(width='500px', height='80px')\n",
                ")\n",
                "\n",
                "length_slider = widgets.IntSlider(value=8, min=4, max=24, step=1, description='Video Length (Frames)')\n",
                "res_dropdown = widgets.Dropdown(options=[256, 512, 768], value=512, description='Resolution')\n",
                "motion_slider = widgets.FloatSlider(value=12.0, min=0.0, max=30.0, step=0.5, description='Motion Strength')\n",
                "steps_slider = widgets.IntSlider(value=50, min=10, max=100, description='Inference Steps')\n",
                "fps_slider = widgets.IntSlider(value=4, min=1, max=12, description='Playback FPS')\n",
                "seed_input = widgets.IntText(value=42, description='Seed')\n",
                "\n",
                "process_btn = widgets.Button(\n",
                "    description='ğŸš€ Generate Video',\n",
                "    button_style='primary',\n",
                "    layout=widgets.Layout(width='500px', height='40px')\n",
                ")\n",
                "\n",
                "output_vram = widgets.Output()\n",
                "output_video = widgets.Output()\n",
                "\n",
                "# â”€â”€ Logic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "def on_preset_change(change):\n",
                "    if change['new']:\n",
                "        prompt_input.value = change['new']\n",
                "\n",
                "preset_dropdown.observe(on_preset_change, names='value')\n",
                "\n",
                "def run_generation(b):\n",
                "    process_btn.disabled = True\n",
                "    process_btn.description = \"â³ Processing... Please wait...\"\n",
                "    \n",
                "    with output_video:\n",
                "        clear_output()\n",
                "        print(\"\\nğŸš€ Initiating generation sequence...\")\n",
                "        \n",
                "        output_path = \"output.mp4\"\n",
                "        try:\n",
                "            video_path = model.process_text2video(\n",
                "                prompt=prompt_input.value,\n",
                "                video_length=length_slider.value,\n",
                "                resolution=res_dropdown.value,\n",
                "                motion_field_strength_x=motion_slider.value,\n",
                "                motion_field_strength_y=motion_slider.value,\n",
                "                seed=seed_input.value,\n",
                "                fps=fps_slider.value,\n",
                "                path=output_path\n",
                "            )\n",
                "            \n",
                "            # Display Video\n",
                "            with open(video_path, \"rb\") as f:\n",
                "                data = f.read()\n",
                "            b64 = base64.b64encode(data).decode()\n",
                "            html = f'''\n",
                "            <div align=\"center\">\n",
                "                <br>\n",
                "                <h4>âœ¨ Generation Complete!</h4>\n",
                "                <video width=\"{res_dropdown.value}\" controls autoplay loop>\n",
                "                    <source src=\"data:video/mp4;base64,{b64}\" type=\"video/mp4\">\n",
                "                </video>\n",
                "            </div>\n",
                "            '''\n",
                "            display(HTML(html))\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"âŒ Error during synthesis: {e}\")\n",
                "        \n",
                "        process_btn.disabled = False\n",
                "        process_btn.description = \"ğŸš€ Generate Video\"\n",
                "        cleanup_vram()\n",
                "\n",
                "process_btn.on_click(run_generation)\n",
                "\n",
                "# â”€â”€ Layout â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "ui = widgets.VBox([\n",
                "    header,\n",
                "    preset_dropdown,\n",
                "    prompt_input,\n",
                "    widgets.HBox([length_slider, res_dropdown]),\n",
                "    widgets.HBox([motion_slider, steps_slider]),\n",
                "    widgets.HBox([fps_slider, seed_input]),\n",
                "    process_btn,\n",
                "    output_vram,\n",
                "    output_video\n",
                "])\n",
                "\n",
                "display(ui)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "references"
            },
            "source": [
                "## ğŸ“š References\n",
                "1. **Text2Video-Zero**: Khachatryan, L. et al. (2023). *Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators.* [arXiv:2303.13439](https://arxiv.org/abs/2303.13439)\n",
                "2. **Dreamlike Photoreal 2.0**: [HuggingFace](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0)\n",
                "3. **Diffusers Framework**: [GitHub](https://github.com/huggingface/diffusers)\n",
                "\n",
                "---\n",
                "\n",
                "*Research Project | University of Windsor | Authors: Amey Thakur*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}